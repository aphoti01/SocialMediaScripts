{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment emotions classifier\n",
    "\n",
    "1. Data pre-processing\n",
    "2. Feature engineering\n",
    "3. Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/antonis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/antonis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/antonis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/antonis/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import xgboost, textblob, string, ekphrasis, nltk, re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "\n",
    "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "sp = SpellCorrector(corpus=\"english\") \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pessimism</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>“Worry is a down payment on a problem you may ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Whatever you decide to do make sure it makes y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@Max_Kellerman  it also helps that the majorit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Accept the challenges so that you can literall...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>My roommate: it's okay that we can't spell bec...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7719</td>\n",
       "      <td>@BadHombreNPS @SecretaryPerry If this didn't m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7720</td>\n",
       "      <td>Excited to watch #stateoforigin tonight! Come ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7721</td>\n",
       "      <td>Blah blah blah Kyrie, IT, etc. @CJC9BOSS leavi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7722</td>\n",
       "      <td>#ThingsIveLearned The wise #shepherd never tru...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7723</td>\n",
       "      <td>I am really flattered and happy to hear those ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7724 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  anger  anticipation  \\\n",
       "0     “Worry is a down payment on a problem you may ...      0             1   \n",
       "1     Whatever you decide to do make sure it makes y...      0             0   \n",
       "2     @Max_Kellerman  it also helps that the majorit...      1             0   \n",
       "3     Accept the challenges so that you can literall...      0             0   \n",
       "4     My roommate: it's okay that we can't spell bec...      1             0   \n",
       "...                                                 ...    ...           ...   \n",
       "7719  @BadHombreNPS @SecretaryPerry If this didn't m...      1             0   \n",
       "7720  Excited to watch #stateoforigin tonight! Come ...      0             0   \n",
       "7721  Blah blah blah Kyrie, IT, etc. @CJC9BOSS leavi...      1             0   \n",
       "7722  #ThingsIveLearned The wise #shepherd never tru...      0             0   \n",
       "7723  I am really flattered and happy to hear those ...      0             0   \n",
       "\n",
       "      disgust  fear  joy  love  optimism  pessimism  sadness  surprise  trust  \n",
       "0           0     0    0     0         1          0        0         0      1  \n",
       "1           0     0    1     1         1          0        0         0      0  \n",
       "2           1     0    1     0         1          0        0         0      0  \n",
       "3           0     0    1     0         1          0        0         0      0  \n",
       "4           1     0    0     0         0          0        0         0      0  \n",
       "...       ...   ...  ...   ...       ...        ...      ...       ...    ...  \n",
       "7719        1     0    0     0         0          0        0         0      0  \n",
       "7720        0     0    1     0         1          0        0         0      0  \n",
       "7721        1     0    0     0         0          0        1         0      0  \n",
       "7722        0     0    0     0         0          0        0         0      0  \n",
       "7723        0     0    1     0         1          0        0         0      0  \n",
       "\n",
       "[7724 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read data\n",
    "train = pd.read_csv('Data/11 emotions (0:1)/2018-E-c-En-train.txt', sep='\\t')\n",
    "valid = pd.read_csv('Data/11 emotions (0:1)/2018-E-c-En-dev.txt', sep='\\t')\n",
    "\n",
    "tweets = train.append(valid)\n",
    "tweets = tweets.reset_index(drop = True)\n",
    "tweets = tweets.drop('ID', axis=1)\n",
    "\n",
    "tweets = tweets.rename(columns = {'Tweet':'text'})\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "1. Normalization, unpacking, tokenizer\n",
    "2. Tweet cleaning, lemmatizer, stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'time', 'date', 'number'],\n",
    "    fix_html=True,  \n",
    "    segmenter=\"twitter\", \n",
    "    corrector=\"twitter\", \n",
    "    unpack_hashtags=True,  \n",
    "    unpack_contractions=True, \n",
    "    spell_correct_elong=True,\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "rooter = nltk.stem.WordNetLemmatizer().lemmatize\n",
    "punctuation = '!\"$%&\\'()*+,-./:;=?[\\\\]^_`{|}~•'\n",
    "\n",
    "def get_word_and_tag(tokens):\n",
    "    tagged = pos_tag(tokens)\n",
    "    cleaned_tags = []\n",
    "    for word, tag in tagged:\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        cleaned_tags.append((word,pos))\n",
    "    return cleaned_tags\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = tweet.lower() # lower case\n",
    "    tweet = emoji.demojize(tweet) #emojis to text\n",
    "    tweet = re.sub('['+punctuation + ']+', ' ', tweet) # remove punctuation\n",
    "    tokens = [word for word in tweet.split(' ') if word not in stopwords] # remove stopwords\n",
    "    tokens = [word for word in tokens if len(word)>0] #remove double spaces\n",
    "    \n",
    "    tokens = [rooter(word,tag) for word,tag in get_word_and_tag(tokens)] # apply word rooter with POS tagging\n",
    "    tweet = ' '.join(tokens)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['corrected_text'] = [\" \".join(text_processor.pre_process_doc(s)) for s in tweets.text]\n",
    "tweets['corrected_text'] = tweets['corrected_text'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train, test sets\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(tweets['corrected_text'], \n",
    "                                    tweets.drop(['text','corrected_text'], axis = 1), test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "1. Count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create count vectors\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{3,}', max_features=5000)\n",
    "count_vect.fit(tweets.corrected_text)\n",
    "\n",
    "x_count = count_vect.transform(tweets.corrected_text)\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, train, train_y, valid, valid_y):\n",
    "    \"\"\"\n",
    "    classifier: Classifier object\n",
    "    train: Train predictors\n",
    "    train_y: Train y\n",
    "    valid: Validation predictors\n",
    "    valid_y: Validation y\n",
    "    \"\"\"\n",
    "    classifier.fit(train, train_y)\n",
    "    preds = classifier.predict(valid)\n",
    "    return 'Accuracy: {}'.format(metrics.accuracy_score(valid_y[emotion], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger Accuracy: 0.7902912621359224\n",
      "anticipation Accuracy: 0.8634304207119741\n",
      "disgust Accuracy: 0.7411003236245954\n",
      "fear Accuracy: 0.8977346278317152\n",
      "joy Accuracy: 0.8187702265372169\n",
      "love Accuracy: 0.9074433656957929\n",
      "optimism Accuracy: 0.7805825242718447\n",
      "pessimism Accuracy: 0.8711974110032362\n",
      "sadness Accuracy: 0.7799352750809061\n",
      "surprise Accuracy: 0.9488673139158577\n",
      "trust Accuracy: 0.940453074433657\n"
     ]
    }
   ],
   "source": [
    "for emotion in emotions:\n",
    "    classifier = linear_model.LogisticRegression()\n",
    "    \n",
    "    print(emotion, train_model(classifier, xtrain_count, train_y[emotion], xvalid_count, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
