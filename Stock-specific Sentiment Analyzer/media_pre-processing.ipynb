{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji, tldextract, xgboost, textblob, string, ekphrasis, nltk, re, gensim\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "\n",
    "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from langdetect import detect\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sp = SpellCorrector(corpus=\"english\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = pd.read_csv('media_sample.csv', index_col=0)\n",
    "media = media[['id','content','created_datetime']]\n",
    "media = media.dropna()\n",
    "media = media.rename({'content':'text'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect language to filter English-only documents\n",
    "l = []\n",
    "for i in media.text:\n",
    "    try:\n",
    "        k = detect(i)\n",
    "        l.append(k)\n",
    "    except:\n",
    "        l.append('en')\n",
    "media['lang'] = l\n",
    "media = media[media.lang == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Text preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'time', 'date', 'number'],\n",
    "    fix_html=True,  \n",
    "    segmenter=\"twitter\", \n",
    "    corrector=\"twitter\", \n",
    "    unpack_hashtags=True,  \n",
    "    unpack_contractions=True, \n",
    "    spell_correct_elong=True,\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "rooter = nltk.stem.WordNetLemmatizer().lemmatize\n",
    "punctuation = '!\"$%&\\'()*+,-./:;=?[\\\\]^_`{|}~•'\n",
    "\n",
    "def get_word_and_tag(tokens):\n",
    "    tagged = pos_tag(tokens)\n",
    "    cleaned_tags = []\n",
    "    for word, tag in tagged:\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        cleaned_tags.append((word,pos))\n",
    "    return cleaned_tags\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = tweet.lower() # lower case\n",
    "    tweet = emoji.demojize(tweet) #emojis to text\n",
    "    tweet = re.sub('['+punctuation + ']+', ' ', tweet) # remove punctuation\n",
    "    tokens = [word for word in tweet.split(' ') if word not in stopwords] # remove stopwords\n",
    "    tokens = [word for word in tokens if len(word)>0] #remove double spaces\n",
    "    \n",
    "    tokens = [rooter(word,tag) for word,tag in get_word_and_tag(tokens)] # apply word rooter with POS tagging\n",
    "    tweet = ' '.join(tokens)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "media['corrected_text'] = [\" \".join(text_processor.pre_process_doc(s)) for s in media.text]\n",
    "media['corrected_text'] = media['corrected_text'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create time-series DataFrame\n",
    "media['datetime'] = pd.to_datetime(media['created_datetime'])\n",
    "media_ts = media.set_index('datetime', drop = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Index data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Change %</th>\n",
       "      <th>datetime</th>\n",
       "      <th>change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-01-27 09:30:00</td>\n",
       "      <td>-2.36%</td>\n",
       "      <td>2020-01-27 09:30:00</td>\n",
       "      <td>-2.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-28 09:30:00</td>\n",
       "      <td>1.87%</td>\n",
       "      <td>2020-01-28 09:30:00</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-29 09:30:00</td>\n",
       "      <td>0.21%</td>\n",
       "      <td>2020-01-29 09:30:00</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-30 09:30:00</td>\n",
       "      <td>0.88%</td>\n",
       "      <td>2020-01-30 09:30:00</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-31 09:30:00</td>\n",
       "      <td>-2.72%</td>\n",
       "      <td>2020-01-31 09:30:00</td>\n",
       "      <td>-2.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-20 09:30:00</td>\n",
       "      <td>-1.78%</td>\n",
       "      <td>2020-04-20 09:30:00</td>\n",
       "      <td>-1.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-21 09:30:00</td>\n",
       "      <td>-4.10%</td>\n",
       "      <td>2020-04-21 09:30:00</td>\n",
       "      <td>-4.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-22 09:30:00</td>\n",
       "      <td>3.87%</td>\n",
       "      <td>2020-04-22 09:30:00</td>\n",
       "      <td>3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-23 09:30:00</td>\n",
       "      <td>-0.69%</td>\n",
       "      <td>2020-04-23 09:30:00</td>\n",
       "      <td>-0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-24 09:30:00</td>\n",
       "      <td>2.11%</td>\n",
       "      <td>2020-04-24 09:30:00</td>\n",
       "      <td>2.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Change %            datetime  change\n",
       "datetime                                                \n",
       "2020-01-27 09:30:00   -2.36% 2020-01-27 09:30:00   -2.36\n",
       "2020-01-28 09:30:00    1.87% 2020-01-28 09:30:00    1.87\n",
       "2020-01-29 09:30:00    0.21% 2020-01-29 09:30:00    0.21\n",
       "2020-01-30 09:30:00    0.88% 2020-01-30 09:30:00    0.88\n",
       "2020-01-31 09:30:00   -2.72% 2020-01-31 09:30:00   -2.72\n",
       "...                      ...                 ...     ...\n",
       "2020-04-20 09:30:00   -1.78% 2020-04-20 09:30:00   -1.78\n",
       "2020-04-21 09:30:00   -4.10% 2020-04-21 09:30:00   -4.10\n",
       "2020-04-22 09:30:00    3.87% 2020-04-22 09:30:00    3.87\n",
       "2020-04-23 09:30:00   -0.69% 2020-04-23 09:30:00   -0.69\n",
       "2020-04-24 09:30:00    2.11% 2020-04-24 09:30:00    2.11\n",
       "\n",
       "[63 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pd.read_csv('S&P 500 Information Technology Historical Data.csv')\n",
    "index['datetime'] = pd.to_datetime(index['Date'])\n",
    "index['datetime'] = [i + timedelta(hours=9.5) for i in index.datetime] #Set index to the time that stock market opens\n",
    "index = index.set_index('datetime', drop = False)\n",
    "index.drop(['Date', 'Price', 'Open', 'High', 'Low', 'Vol.'], axis=1, inplace=True)\n",
    "index = index['2020-04-24':'2020-01-27']\n",
    "index = index.sort_index(ascending=True)\n",
    "index['change'] = [float(i[:-1]) for i in index['Change %']]\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Calculation of sentiment\n",
    "\n",
    "* Build vocabulary\n",
    "* Run algorithm\n",
    "* Accumulate Word sentiment\n",
    "* Calculate Article sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_range(date, n):\n",
    "    \"\"\"\n",
    "    date: Date of stock price\n",
    "    n: Range of days\n",
    "    returns: String representation of date range.\n",
    "    \"\"\"\n",
    "    return str(date - timedelta(days = n+1)), str(date - timedelta(days = n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(l):\n",
    "    #Get vocabulary from corpus\n",
    "    t = [i.split() for i in l]\n",
    "    return list(set([i for sl in t for i in sl]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = get_vocab(media.corrected_text)\n",
    "\n",
    "d = dict()\n",
    "av_d = dict()\n",
    "\n",
    "#Initialize word dictionaries\n",
    "#Each word is represented by a list of stock market changes (d) and the average change (av_d)\n",
    "for w in word_list:\n",
    "    d[w] = []\n",
    "    av_d[w] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=3 \n",
    "\"\"\"\n",
    "Number of days before\n",
    "#e.g. when n=3, the sentiment lexicon will be built based on documents that were published within \n",
    "#3 days before the openning of the stock market at the day in study\n",
    "\"\"\"\n",
    "\n",
    "prev = str(min(media.datetime)) #Date of first document\n",
    "\n",
    "for i, row in index.iterrows():\n",
    "    change = row['change']\n",
    "    r = get_range(row['datetime'], n)\n",
    "    sl = media_ts[prev:r[1]] #Subset documents based on date range\n",
    "    \n",
    "    words = get_vocab(sl.corrected_text) #Get vocabulary of subset\n",
    "    \n",
    "    for word in words:\n",
    "        d[word].append(change)\n",
    "        \n",
    "    prev = r[1]\n",
    "\n",
    "for word in d: #Calculate average fluctuations n days after publishing an article which contains the word\n",
    "    av_d[word] = np.mean(d[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_from_text(t):\n",
    "    \"\"\"\n",
    "    t: Text of document\n",
    "    Tokenizes the document and calculates the average sentiment of the words consisting the document\n",
    "    \"\"\"\n",
    "    tokenized = t.split()\n",
    "    sentiment = 0\n",
    "    for i in tokenized:\n",
    "        sentiment+=av_d[i]\n",
    "        \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "media['sentiment_index'] = [get_sentiment_from_text(i) for i in media.corrected_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_datetime</th>\n",
       "      <th>lang</th>\n",
       "      <th>corrected_text</th>\n",
       "      <th>datetime</th>\n",
       "      <th>sentiment_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>46534906195</td>\n",
       "      <td>FarmerHarv wrote: ↑ Jan 26th, 2020 11:04 am Wh...</td>\n",
       "      <td>2020-01-30 21:59:00</td>\n",
       "      <td>en</td>\n",
       "      <td>farmerharv write ↑ &lt;date&gt; &lt;time&gt; market impact...</td>\n",
       "      <td>2020-01-30 21:59:00</td>\n",
       "      <td>5.394955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>46534908104</td>\n",
       "      <td>10 am Eastern start today in case folks want t...</td>\n",
       "      <td>2020-02-21 15:26:29</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;number&gt; eastern start today case folk want op...</td>\n",
       "      <td>2020-02-21 15:26:29</td>\n",
       "      <td>-6.780057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>46534908442</td>\n",
       "      <td>Beijing, Jan 31 The World Health Organization ...</td>\n",
       "      <td>2020-01-31 02:40:55</td>\n",
       "      <td>en</td>\n",
       "      <td>beijing &lt;date&gt; world health organization decla...</td>\n",
       "      <td>2020-01-31 02:40:55</td>\n",
       "      <td>-15.527924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>46534909270</td>\n",
       "      <td>Quote: Originally Posted by Mikala43 Has anyon...</td>\n",
       "      <td>2020-01-31 14:48:00</td>\n",
       "      <td>en</td>\n",
       "      <td>quote originally post mikala43 anyone see resp...</td>\n",
       "      <td>2020-01-31 14:48:00</td>\n",
       "      <td>4.063406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>46534911681</td>\n",
       "      <td>I wanted to touch base on how the lack of expe...</td>\n",
       "      <td>2020-02-22 17:20:38</td>\n",
       "      <td>en</td>\n",
       "      <td>want touch base lack expertise professionalism...</td>\n",
       "      <td>2020-02-22 17:20:38</td>\n",
       "      <td>-50.752430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9993</td>\n",
       "      <td>48223491438</td>\n",
       "      <td>In this QUAH Sal, Adam, &amp;amp; Justin answer th...</td>\n",
       "      <td>2020-04-16 11:55:32</td>\n",
       "      <td>en</td>\n",
       "      <td>quah sal adam justin answer question “ somethi...</td>\n",
       "      <td>2020-04-16 11:55:32</td>\n",
       "      <td>-6.290081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9994</td>\n",
       "      <td>48223499707</td>\n",
       "      <td>Ex-footballer Mark Lawrenson, 62, claimed the ...</td>\n",
       "      <td>2020-04-19 09:48:06</td>\n",
       "      <td>en</td>\n",
       "      <td>ex footballer mark lawrenson &lt;number&gt; claim br...</td>\n",
       "      <td>2020-04-19 09:48:06</td>\n",
       "      <td>-9.857453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>48223759008</td>\n",
       "      <td>Update from Slovenia 1335 cases (+5), 77 death...</td>\n",
       "      <td>2020-04-20 20:03:43</td>\n",
       "      <td>en</td>\n",
       "      <td>update slovenia &lt;number&gt; case &lt;number&gt; &lt;number...</td>\n",
       "      <td>2020-04-20 20:03:43</td>\n",
       "      <td>6.167458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>48223762895</td>\n",
       "      <td>The Tories have backed themselves into a corne...</td>\n",
       "      <td>2020-04-20 15:09:30</td>\n",
       "      <td>en</td>\n",
       "      <td>tory back corner inaction negligence manage ro...</td>\n",
       "      <td>2020-04-20 15:09:30</td>\n",
       "      <td>8.602135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>48224122689</td>\n",
       "      <td>SU plans pay cuts, hiring freeze after $35M lo...</td>\n",
       "      <td>2020-04-20 15:23:48</td>\n",
       "      <td>en</td>\n",
       "      <td>su plan pay cut hire freeze &lt;money&gt; loss coron...</td>\n",
       "      <td>2020-04-20 15:23:48</td>\n",
       "      <td>-8.270611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4586 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                               text  \\\n",
       "1     46534906195  FarmerHarv wrote: ↑ Jan 26th, 2020 11:04 am Wh...   \n",
       "2     46534908104  10 am Eastern start today in case folks want t...   \n",
       "3     46534908442  Beijing, Jan 31 The World Health Organization ...   \n",
       "4     46534909270  Quote: Originally Posted by Mikala43 Has anyon...   \n",
       "5     46534911681  I wanted to touch base on how the lack of expe...   \n",
       "...           ...                                                ...   \n",
       "9993  48223491438  In this QUAH Sal, Adam, &amp; Justin answer th...   \n",
       "9994  48223499707  Ex-footballer Mark Lawrenson, 62, claimed the ...   \n",
       "9995  48223759008  Update from Slovenia 1335 cases (+5), 77 death...   \n",
       "9996  48223762895  The Tories have backed themselves into a corne...   \n",
       "9998  48224122689  SU plans pay cuts, hiring freeze after $35M lo...   \n",
       "\n",
       "         created_datetime lang  \\\n",
       "1     2020-01-30 21:59:00   en   \n",
       "2     2020-02-21 15:26:29   en   \n",
       "3     2020-01-31 02:40:55   en   \n",
       "4     2020-01-31 14:48:00   en   \n",
       "5     2020-02-22 17:20:38   en   \n",
       "...                   ...  ...   \n",
       "9993  2020-04-16 11:55:32   en   \n",
       "9994  2020-04-19 09:48:06   en   \n",
       "9995  2020-04-20 20:03:43   en   \n",
       "9996  2020-04-20 15:09:30   en   \n",
       "9998  2020-04-20 15:23:48   en   \n",
       "\n",
       "                                         corrected_text            datetime  \\\n",
       "1     farmerharv write ↑ <date> <time> market impact... 2020-01-30 21:59:00   \n",
       "2     <number> eastern start today case folk want op... 2020-02-21 15:26:29   \n",
       "3     beijing <date> world health organization decla... 2020-01-31 02:40:55   \n",
       "4     quote originally post mikala43 anyone see resp... 2020-01-31 14:48:00   \n",
       "5     want touch base lack expertise professionalism... 2020-02-22 17:20:38   \n",
       "...                                                 ...                 ...   \n",
       "9993  quah sal adam justin answer question “ somethi... 2020-04-16 11:55:32   \n",
       "9994  ex footballer mark lawrenson <number> claim br... 2020-04-19 09:48:06   \n",
       "9995  update slovenia <number> case <number> <number... 2020-04-20 20:03:43   \n",
       "9996  tory back corner inaction negligence manage ro... 2020-04-20 15:09:30   \n",
       "9998  su plan pay cut hire freeze <money> loss coron... 2020-04-20 15:23:48   \n",
       "\n",
       "      sentiment_index  \n",
       "1            5.394955  \n",
       "2           -6.780057  \n",
       "3          -15.527924  \n",
       "4            4.063406  \n",
       "5          -50.752430  \n",
       "...               ...  \n",
       "9993        -6.290081  \n",
       "9994        -9.857453  \n",
       "9995         6.167458  \n",
       "9996         8.602135  \n",
       "9998        -8.270611  \n",
       "\n",
       "[4586 rows x 7 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
