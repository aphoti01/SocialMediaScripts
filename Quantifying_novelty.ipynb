{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantifying Novelty\n",
    "\n",
    "The uniqueness of information contained in a tweet is calculated by comparing a tweet with the tweets posted in the previous days. The task starts with text pre-processing and cleaning. The tweets are summarized into vectors using the Doc2Vec algorithm. Finally, cosine similarity is used to calculate the novelty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost, textblob, string, ekphrasis, nltk, re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sp = SpellCorrector(corpus=\"english\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('tweets.csv', index_col=0)\n",
    "tweets['datetime'] = pd.to_datetime(tweets['created_at'])\n",
    "tweets = tweets.set_index('datetime')\n",
    "tweets.drop(['created_at', 'status_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "1. Text pre-processing (spelling correction, unpack hashtags, tokenization)\n",
    "2. Stop-word and punctuation removal\n",
    "3. Word stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'time', 'date', 'number'],\n",
    "    fix_html=True,  \n",
    "    segmenter=\"twitter\", \n",
    "    corrector=\"twitter\", \n",
    "    unpack_hashtags=True,  \n",
    "    unpack_contractions=True, \n",
    "    spell_correct_elong=True,\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem \n",
    "punctuation = '!\"$%&\\'()*+,-./:;=?[\\\\]^_`{|}~â€¢'\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub('['+punctuation + ']+', ' ', tweet)\n",
    "    tokens = [word for word in tweet.split(' ') if word not in stopwords]\n",
    "    tokens = [word_rooter(word) if '#' not in word else word for word in tokens]\n",
    "    if bigrams:\n",
    "        tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
    "                                            for i in range(len(tweet_token_list)-1)]\n",
    "    tweet = ' '.join(tokens)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['corrected_text'] = [\" \".join(text_processor.pre_process_doc(s)) for s in tweets.text]\n",
    "tweets['corrected_text'] = tweets['corrected_text'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec\n",
    "1. Assign unique labels to tweets\n",
    "2. Initialize and train model\n",
    "3. Get tweet vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, label):\n",
    "    \"\"\"\n",
    "    model: Doc2Vec trained model object\n",
    "    coprus_size: Size of corpus\n",
    "    vectors_size: Size of vectors\n",
    "    label: Label prefix used in labeling the corpus\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = label + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_tweets(tweets, label):\n",
    "    \"\"\"\n",
    "    tweets: Tweets corpus\n",
    "    label: Label prefix to be used\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(tweets):\n",
    "        temp = label + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), [temp]))\n",
    "    return labeled\n",
    "\n",
    "labeled = label_sentences(tweets.corrected_text, 'Full')\n",
    "\n",
    "tweets['tags'] = [i[1][0] for i in labeled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d2v = Doc2Vec(dm=0, vector_size=300, window=5, negative=5, min_count=10, alpha=0.065, min_alpha=0.065)\n",
    "d2v.build_vocab([x for x in tqdm(all_data)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    d2v.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    d2v.alpha -= 0.002\n",
    "    d2v.min_alpha = d2v.alpha\n",
    "\n",
    "tweets_vectors = get_vectors(d2v, len(all_data), 300, 'Full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate  novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine similarity\n",
    "def similarity(a, b):\n",
    "    \"\"\"\n",
    "    t1: First tweet's unique tag\n",
    "    t2: Second tweet's unique tag\n",
    "    \"\"\"\n",
    "    t1 = model_dbow.docvecs[a] #Vector of first tweet\n",
    "    t2 = model_dbow.docvecs[b] #Vector of second tweet\n",
    "    return dot(t1, t2)/(norm(t1)*norm(t2)) #Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get period range for tweets to compare\n",
    "def get_period(t, days):\n",
    "    \"\"\"\n",
    "    t: Tweet's unique tag\n",
    "    days: Days before the tweet to compare\n",
    "    \"\"\"\n",
    "    to = str(tweets[tweets.tags == t].index[0]) #date of tweet\n",
    "    f = str(tweets[tweets.tags == t].index[0] - timedelta(days = days)) \n",
    "    return to, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarities(t, days, sample):\n",
    "    \"\"\"\n",
    "    t: Tweet's unique tag\n",
    "    days: Days before the tweet to compare\n",
    "    sample: Sample of tweets to compare\n",
    "    \"\"\"\n",
    "    to, f = get_period(t, days)\n",
    "    temp = tweets[f:to] # Subset tweets by datetime index\n",
    "    if temp.shape[0]>sample: #Take random sample\n",
    "        temp = temp.sample(sample)\n",
    "    temp = temp[temp.tags!=t] #Remove examining tweet from corpus\n",
    "    \n",
    "    l = [similarity(t,i) for i in temp.tags] #Get similarity vector\n",
    "    \n",
    "    return np.mean(l) #Return average similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['1day_similarity'] = [get_similarities(i, 1, 1000) for i in tweets.tags]\n",
    "tweets['2day_similarity'] = [get_similarities(i, 2, 1000) for i in tweets.tags]\n",
    "tweets['3day_similarity'] = [get_similarities(i, 3, 1000) for i in tweets.tags]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
